# Lightning Distributed Training

Qwen3 0.6B λ¨λΈμ„ PyTorch Lightningκ³Ό Lightning FabricμΌλ΅ λ¶„μ‚°ν•™μµν•λ” μ½”λ“μ…λ‹λ‹¤.

## λΉ λ¥Έ μ‹μ‘

### PyTorch Lightning (μλ™ν™”λ ν•™μµ)
```bash
# Slurm λ¶„μ‚° ν•™μµ
sbatch train.sbatch

# λ‹¨μΌ λ…Έλ“ ν•™μµ μμ‹
python train.py --gpus=8 --local_dataset --dataset="/fsx/data/pretrain/wikitext-2" --save_every_n_steps=50 --val_check_interval=50 --max_steps=100
```

### Lightning Fabric (μ„Έλ°€ν• μ μ–΄)
```bash
# Slurm λ¶„μ‚° ν•™μµ
sbatch train_fabric.sbatch

# λ‹¨μΌ λ…Έλ“ ν•™μµ μμ‹
python train_fabric.py --gpus=8 --local_dataset --dataset="/fsx/data/pretrain/wikitext-2" --save_every_n_steps=50 --max_steps=100
```

## μ£Όμ” κΈ°λ¥

- **λ‘ κ°€μ§€ λ°©μ‹**: PyTorch Lightning (μλ™ν™”) vs Lightning Fabric (μλ™ μ μ–΄)
- **μ‹¤μ  λ°μ΄ν„°μ…‹**: HuggingFace λ°μ΄ν„°μ…‹ λλ” λ΅μ»¬ λ°μ΄ν„°μ…‹ μ§€μ›
- **λ¶„μ‚° ν•™μµ**: FSDPλ΅ λ©€ν‹°λ…Έλ“/λ©€ν‹°GPU μ§€μ›
- **ν¨μ¨μ  μ²λ¦¬**: ConcatTokensDatasetμΌλ΅ ν† ν° μ—°κ²°
- **Mixed Precision**: 16-bitλ΅ λ©”λ¨λ¦¬ μ μ•½
- **μ²΄ν¬ν¬μΈνΈ μλ™ λ΅λ“**: ν•™μµ μ¤‘λ‹¨ μ‹ μλ™ μ¬μ‹μ‘
- **μƒμ„Έν• λ΅κΉ…**: Loss, Grad Norm, LR, μ²λ¦¬λ‰ λ“±
- **Slurm μ§€μ›**: λ©€ν‹°λ…Έλ“ ν΄λ¬μ¤ν„° ν•™μµ

## π“‹ μ‚¬μ©λ²•

### PyTorch Lightning
```bash
python train.py \
    --nodes=1 \
    --gpus=8 \
    --epochs=3 \
    --batch_size=2 \
    --dataset="wikitext" \
    --model_name="Qwen/Qwen3-0.6B"
```

### Lightning Fabric
```bash
python train_fabric.py \
    --nodes=1 \
    --gpus=8 \
    --max_steps=1000 \
    --batch_size=2 \
    --dataset="/fsx/data/pretrain/wikitext-2" \
    --local_dataset
```

### Slurm λ°°μΉ μ‘μ—…
```bash
# PyTorch Lightning
sbatch train.sbatch

# Lightning Fabric
sbatch train_fabric.sbatch
```

## νλΌλ―Έν„°

| νλΌλ―Έν„° | κΈ°λ³Έκ°’ | μ„¤λ… |
|---------|--------|------|
| `--nodes` | 1 | λ…Έλ“ μ |
| `--gpus` | 1 | GPU μ |
| `--epochs` | 1 | μ—ν¬ν¬ μ (Lightningλ§) |
| `--max_steps` | 100 | μµλ€ μ¤ν… μ |
| `--batch_size` | 4 | λ°°μΉ ν¬κΈ° |
| `--dataset` | "wikitext" | λ°μ΄ν„°μ…‹ μ΄λ¦„ |
| `--model_name` | "Qwen/Qwen3-0.6B" | λ¨λΈ μ΄λ¦„ |
| `--max_length` | 512 | μµλ€ μ‹ν€€μ¤ κΈΈμ΄ |
| `--learning_rate` | 5e-5 | ν•™μµλ¥  |
| `--local_dataset` | False | λ΅μ»¬ λ°μ΄ν„°μ…‹ μ‚¬μ© |
| `--save_every_n_steps` | 100 | μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ£ΌκΈ° |
| `--checkpoint_dir` | "./checkpoints" | μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ |

## νμΌ κµ¬μ΅°

```
lightning/
β”β”€β”€ train.py              # PyTorch Lightning ν•™μµ μ¤ν¬λ¦½νΈ
β”β”€β”€ train_fabric.py       # Lightning Fabric ν•™μµ μ¤ν¬λ¦½νΈ
β”β”€β”€ train.sbatch          # PyTorch Lightning Slurm μ¤ν¬λ¦½νΈ
β”β”€β”€ train_fabric.sbatch   # Lightning Fabric Slurm μ¤ν¬λ¦½νΈ
β””β”€β”€ README.md             # μ΄ νμΌ
```

## PyTorch Lightning vs Lightning Fabric

### PyTorch Lightning
- **μ¥μ **: μλ™ν™”λ ν•™μµ λ£¨ν”„, μ½λ°±, λ΅κΉ…
- **λ‹¨μ **: μ ν•λ μ»¤μ¤ν„°λ§μ΄μ§•
- **μ ν•©ν• κ²½μ°**: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘, ν‘μ¤€μ μΈ ν•™μµ

### Lightning Fabric  
- **μ¥μ **: μ„Έλ°€ν• μ μ–΄, μ»¤μ¤ν…€ ν•™μµ λ£¨ν”„
- **λ‹¨μ **: μλ™ κµ¬ν„ ν•„μ”
- **μ ν•©ν• κ²½μ°**: λ³µμ΅ν• ν•™μµ λ΅μ§, μ—°κµ¬μ©

## μ²΄ν¬ν¬μΈνΈ

- μλ™ μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ° λ΅λ“
- `latest.txt`μ— μµμ‹  μ²΄ν¬ν¬μΈνΈ κ²½λ΅ μ €μ¥
- λ¶„μ‚° μ²΄ν¬ν¬μΈνΈλ΅ λ©”λ¨λ¦¬ ν¨μ¨μ  μ €μ¥
- ν•™μµ μ¤‘λ‹¨ μ‹ μλ™ μ¬μ‹μ‘

## λ΅κΉ…

### PyTorch Lightning
- μλ™ λ΅κΉ… (train_loss, val_loss)
- TensorBoard μ§€μ›
- Progress bar

### Lightning Fabric
- μ»¤μ¤ν…€ λ΅κΉ…
- Loss, Gradient Norm, Learning Rate
- μ²λ¦¬λ‰ (samples/sec)
- μ§„ν–‰λ¥  (STEP x/y)

## References

- [PyTorch Lightning Documentation](https://lightning.ai/docs/pytorch/stable/)
- [Lightning Fabric Documentation](https://lightning.ai/docs/fabric/stable/)
- [Qwen3-0.6B Model](https://huggingface.co/Qwen/Qwen3-0.6B)
- [FSDP Strategy Guide](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html)
- [Distributed Checkpoints](https://lightning.ai/docs/fabric/stable/guide/checkpoint/distributed_checkpoint.html)
- [SLURM Cluster Training](https://lightning.ai/docs/pytorch/stable/clouds/cluster_advanced.html)
