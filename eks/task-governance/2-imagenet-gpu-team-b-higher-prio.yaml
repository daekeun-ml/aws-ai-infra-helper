apiVersion: v1
kind: Service
metadata:
  name: etcd-gpu
  namespace: hyperpod-ns-team-b
  labels:
      kueue.x-k8s.io/queue-name: hyperpod-ns-team-b-localqueue
      kueue.x-k8s.io/priority-class: experimentation-priority
spec:
  ports:
    - name: etcd-client-port
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    app: etcd-gpu

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: etcd-gpu
    kueue.x-k8s.io/queue-name: hyperpod-ns-team-b-localqueue
    kueue.x-k8s.io/priority-class: experimentation-priority
  name: etcd-gpu
  namespace: hyperpod-ns-team-b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd-gpu
  template:
    metadata:
      labels:
        app: etcd-gpu
        kueue.x-k8s.io/queue-name: hyperpod-ns-team-b-localqueue
        kueue.x-k8s.io/priority-class: experimentation-priority
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: ml.g5.12xlarge
      containers:
        - name: etcd
          command: ["/usr/local/bin/etcd"]
          args:
            - "--data-dir"
            - "/var/lib/etcd"
            - "--enable-v2"
            - "--listen-client-urls"
            - "http://0.0.0.0:2379"
            - "--advertise-client-urls"
            - "http://0.0.0.0:2379"
            - "--initial-cluster-state"
            - "new"
          image: quay.io/coreos/etcd:v3.5.19
          ports:
            - containerPort: 2379
              name: client
              protocol: TCP
            - containerPort: 2380
              name: server
              protocol: TCP
      restartPolicy: Always
---
# Example from: https://github.com/kubeflow/training-operator/tree/master/examples/pytorch/elastic/imagenet
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: imagenet-gpu-team-b-2
  namespace: hyperpod-ns-team-b
  labels:
      kueue.x-k8s.io/queue-name: hyperpod-ns-team-b-localqueue
      kueue.x-k8s.io/priority-class: experimentation-priority
  annotations: {
    sagemaker.amazonaws.com/enable-job-auto-resume: "true",
    sagemaker.amazonaws.com/job-max-retry-count: "2"
  }
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd-gpu
    rdzvPort: 2379
    minReplicas: 1
    maxReplicas: 36
    maxRestarts: 200
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 80
  pytorchReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: imagenet
            kueue.x-k8s.io/queue-name: hyperpod-ns-team-b-localqueue
            kueue.x-k8s.io/priority-class: experimentation-priority
        spec:
          nodeSelector:
            node.kubernetes.io/instance-type: "ml.g5.12xlarge"
          # Added hostIPC for NCCL shared memory access
          hostIPC: true
          affinity:
            podAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - imagenet
                topologyKey: "topology.kubernetes.io/zone"
          containers:
            - name: pytorch
              image: kubeflow/pytorch-elastic-example-imagenet:latest
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              env:
              - name: LOGLEVEL
                value: DEBUG
              # Added NCCL debug environment variable
              - name: NCCL_DEBUG
                value: INFO
              command:
                - bash
                - -c
                - "wget https://raw.githubusercontent.com/pytorch/elastic/master/examples/imagenet/main.py -O /workspace/examples/imagenet.py; /usr/local/bin/torchrun --nproc_per_node=4 --nnodes=2 /workspace/examples/imagenet.py --arch=resnet18 --epochs=50 --batch-size=32 --workers=0 --checkpoint-file=/workspace/checkpoint.pth.tar /workspace/data/tiny-imagenet-200"
              # Added shared memory volume mount for NCCL
              volumeMounts:
              - name: shm
                mountPath: /dev/shm
          # Added shared memory volume for NCCL communication
          volumes:
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: 1Gi

