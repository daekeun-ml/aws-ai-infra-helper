# HyperPod EKS Fine-tuning Hands-on

AWS SageMaker HyperPod EKS í´ëŸ¬ìŠ¤í„°ì—ì„œ DeepSeek-R1-Distill-Qwen-1.5B ëª¨ë¸ì„ LoRA Fine-tuningí•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- AWS CLI ì„¤ì¹˜ ë° êµ¬ì„± ì™„ë£Œ
- HyperPod EKS í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•¨
- GPU ë…¸ë“œ (g5.8xlarge ì´ìƒ) 2ê°œ ì´ìƒ
- Kubeflow Training Operator ì„¤ì¹˜ ì™„ë£Œ (HyperPod ê¸°ë³¸ ì œê³µ)
- NVIDIA Device Plugin ì„¤ì¹˜ ì™„ë£Œ (HyperPod ê¸°ë³¸ ì œê³µ)

---

## ğŸ“‹ ì „ì²´ ì‹¤ìŠµ ìˆœì„œ

ì´ ì‹¤ìŠµì€ `eks/setup` í´ë”ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•œ í›„ ì§„í–‰í•©ë‹ˆë‹¤.

```
eks/setup/                              eks/training/
â”œâ”€â”€ 1.create-config-workshop.sh   â†’     â”œâ”€â”€ 1.grant_eks_access.sh
â”œâ”€â”€ 2.setup-eks-access.sh               â”œâ”€â”€ 2.run_training.sh
â””â”€â”€ 3.validate-cluster.sh               â”œâ”€â”€ 3.monitor_training.sh
                                        â””â”€â”€ 4.cleanup.sh
```

### Step 0: í™˜ê²½ ì„¤ì • (ìµœì´ˆ 1íšŒ)

```bash
# setup í´ë”ë¡œ ì´ë™
cd ../setup

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (env_vars íŒŒì¼ ìƒì„±)
./1.create-config-workshop.sh

# 2. EKS ì ‘ê·¼ ê¶Œí•œ ì„¤ì •
./2.setup-eks-access.sh

# 3. í´ëŸ¬ìŠ¤í„° ê²€ì¦
./3.validate-cluster.sh

# training í´ë”ë¡œ ëŒì•„ì˜¤ê¸°
cd ../training
```

> **Note**: `1.create-config-workshop.sh` ì‹¤í–‰ ì‹œ ìƒì„±ë˜ëŠ” `env_vars` íŒŒì¼ì€ ì´í›„ ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (ìë™í™” ìŠ¤í¬ë¦½íŠ¸)

### 1. í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ ì„¤ì •

```bash
./1.grant_eks_access.sh
```

- `../setup/env_vars` íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ìë™ ë¡œë“œ
- EKS í´ëŸ¬ìŠ¤í„° ìë™ ê°ì§€ (ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ì¸ ê²½ìš°)
- EKS Access Entry ìë™ ìƒì„± ë° ê¶Œí•œ ë¶€ì—¬
- kubeconfig ìë™ ì„¤ì •
- GPU ë…¸ë“œ ë° í•„ìˆ˜ êµ¬ì„±ìš”ì†Œ í™•ì¸

> **Note**: í´ëŸ¬ìŠ¤í„°ë¥¼ ì§ì ‘ ì§€ì •í•˜ë ¤ë©´: `./1.grant_eks_access.sh [CLUSTER_NAME] [REGION]`

### 2. Fine-tuning ì‹¤í–‰

```bash
./2.run_training.sh
```

- ê¸°ì¡´ PyTorchJob í™•ì¸ ë° ì •ë¦¬
- PyTorchJob ë°°í¬
- Pod ìƒì„± ìƒíƒœ ëª¨ë‹ˆí„°ë§

### 3. í•™ìŠµ ëª¨ë‹ˆí„°ë§

```bash
./3.monitor_training.sh
```

- ì‹¤ì‹œê°„ í•™ìŠµ ë¡œê·¸ ì¶œë ¥
- í•™ìŠµ ì§„í–‰ë¥  í™•ì¸
- Job ìƒíƒœ í™•ì¸

### 4. ë¦¬ì†ŒìŠ¤ ì •ë¦¬

```bash
./4.cleanup.sh
```

- PyTorchJob ì‚­ì œ
- ê´€ë ¨ Pod ì •ë¦¬

---

## ğŸ“– ìƒì„¸ ê°€ì´ë“œ (ìˆ˜ë™ Step-by-Step)

ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  ê° ë‹¨ê³„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´í•´í•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ê°€ì´ë“œë¥¼ ë”°ë¥´ì„¸ìš”.

### Step 1: EKS í´ëŸ¬ìŠ¤í„° ì ‘ì† ì„¤ì •

```bash
# kubeconfig ì„¤ì •
aws eks update-kubeconfig --name "YOUR_EKS_CLUSTER_NAME" --region us-west-2

# í´ëŸ¬ìŠ¤í„° ì—°ê²° í™•ì¸
kubectl get nodes
```

### Step 2: GPU ë…¸ë“œ í™•ì¸

```bash
kubectl get nodes -o custom-columns="NAME:.metadata.name,INSTANCE-TYPE:.metadata.labels.node\.kubernetes\.io/instance-type,GPU:.status.capacity.nvidia\.com/gpu"
```

ì¶œë ¥ ì˜ˆì‹œ:
```
NAME                           INSTANCE-TYPE   GPU
hyperpod-i-05e4de3dcf4135f28   ml.g5.8xlarge   1
hyperpod-i-0abbe2a2c165f4a8f   ml.g5.8xlarge   1
```

### Step 3: í•„ìˆ˜ êµ¬ì„±ìš”ì†Œ í™•ì¸

```bash
# Kubeflow Training Operator í™•ì¸
kubectl get pods -n kubeflow

# NVIDIA Device Plugin í™•ì¸
kubectl get pods -n kube-system | grep nvidia
```

### Step 4: PyTorchJob ë°°í¬

```bash
kubectl apply -f template/pytorchjob_finetuning.yaml
```

> **Note**: ì´ ê°€ì´ë“œëŠ” FSxê°€ ì—†ëŠ” í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤. ëª¨ë¸ê³¼ ë°ì´í„°ëŠ” HuggingFaceì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.

### Step 5: Pod ìƒíƒœ í™•ì¸

```bash
# Pod ìƒì„± ìƒíƒœ í™•ì¸
kubectl get pods -l training.kubeflow.org/job-name=deepseek-finetuning

# ìƒì„¸ ì´ë²¤íŠ¸ í™•ì¸ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë“±)
kubectl describe pod deepseek-finetuning-worker-0
```

ì¶œë ¥ ì˜ˆì‹œ:
```
NAME                           READY   STATUS    RESTARTS   AGE
deepseek-finetuning-worker-0   1/1     Running   0          2m
deepseek-finetuning-worker-1   1/1     Running   0          2m
```

### Step 6: í•™ìŠµ ë¡œê·¸ í™•ì¸

```bash
# Worker 0 ë¡œê·¸ í™•ì¸
kubectl logs -f deepseek-finetuning-worker-0
```

ì¶œë ¥ ì˜ˆì‹œ:
```
trainable params: 4,358,144 || all params: 1,781,446,144 || trainable%: 0.2446
Loading training data...
Starting training...
{'loss': 14.4461, 'grad_norm': 0.165, 'learning_rate': 3.6e-05, 'epoch': 0.09}
...
```

### Step 7: PyTorchJob ìƒíƒœ í™•ì¸

```bash
kubectl get pytorchjob deepseek-finetuning
```

ì¶œë ¥ ì˜ˆì‹œ:
```
NAME                  STATE       AGE
deepseek-finetuning   Succeeded   7m
```

### Step 8: ë¦¬ì†ŒìŠ¤ ì •ë¦¬

```bash
kubectl delete pytorchjob deepseek-finetuning
```

---

## ğŸ“Š í•™ìŠµ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

### GPU ìˆ˜ ë° ì›Œì»¤ ìˆ˜ ë³€ê²½

`template/pytorchjob_finetuning.yaml`ì—ì„œ ë‹¤ìŒ ê°’ì„ ìˆ˜ì •í•©ë‹ˆë‹¤:

```yaml
spec:
  nprocPerNode: "1"        # ë…¸ë“œë‹¹ GPU ìˆ˜
  pytorchReplicaSpecs:
    Worker:
      replicas: 2          # ì›Œì»¤ ë…¸ë“œ ìˆ˜
```

### LoRA íŒŒë¼ë¯¸í„° ìˆ˜ì •

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë‚´ LoRA ì„¤ì •:

```python
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,                    # LoRA rank (ë†’ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì¦ê°€)
    lora_alpha=32,           # LoRA alpha
    lora_dropout=0.05,       # Dropout ë¹„ìœ¨
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)
```

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ì •

```python
training_args = TrainingArguments(
    num_train_epochs=1,              # í•™ìŠµ epoch ìˆ˜
    per_device_train_batch_size=2,   # GPUë‹¹ ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=4,   # Gradient ëˆ„ì  ìŠ¤í…
    learning_rate=2e-4,              # í•™ìŠµë¥ 
    warmup_steps=50,                 # Warmup ìŠ¤í…
)
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### kubectl ì—°ê²° ì•ˆë¨

```bash
aws eks update-kubeconfig --name [CLUSTER_NAME] --region us-west-2
```

### Podê°€ Pending ìƒíƒœì¼ ë•Œ

```bash
kubectl describe pod deepseek-finetuning-worker-0
```

ì¼ë°˜ì ì¸ ì›ì¸:
- GPU ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
- ì´ë¯¸ì§€ Pull ì‹¤íŒ¨
- ë…¸ë“œ ì„ íƒ ì¡°ê±´ ë¶ˆì¼ì¹˜

### OOM (Out of Memory) ì˜¤ë¥˜

ë°°ì¹˜ í¬ê¸° ë˜ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¤„ì´ì„¸ìš”:

```python
per_device_train_batch_size=1    # 2ì—ì„œ 1ë¡œ ê°ì†Œ
max_length=256                    # 512ì—ì„œ 256ìœ¼ë¡œ ê°ì†Œ
```

### ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œê°€ ëŠë¦° ê²½ìš°

`huggingface/transformers-pytorch-gpu:latest` ì´ë¯¸ì§€ëŠ” ì•½ 10GBì…ë‹ˆë‹¤. ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œì— 5-10ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸
kubectl describe pod deepseek-finetuning-worker-0 | grep -A5 "Events:"
```

### PyTorchJob ì‹¤íŒ¨

```bash
# ìƒì„¸ ë¡œê·¸ í™•ì¸
kubectl describe pytorchjob deepseek-finetuning
kubectl logs deepseek-finetuning-worker-0
```

---

## ğŸ“‹ ìœ ìš©í•œ ëª…ë ¹ì–´

```bash
# ëª¨ë“  ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods,pytorchjob

# Pod ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
kubectl logs -f deepseek-finetuning-worker-0

# ëª¨ë“  ì›Œì»¤ ë¡œê·¸ í™•ì¸
for i in 0 1; do
  echo "=== Worker $i ==="
  kubectl logs deepseek-finetuning-worker-$i --tail=20
done

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
kubectl exec deepseek-finetuning-worker-0 -- nvidia-smi

# PyTorchJob ìƒì„¸ ì •ë³´
kubectl describe pytorchjob deepseek-finetuning
```

---

## ğŸ“š ì°¸ê³  ì •ë³´

### í•™ìŠµ êµ¬ì„±

| í•­ëª© | ê°’ |
|------|-----|
| ëª¨ë¸ | DeepSeek-R1-Distill-Qwen-1.5B |
| í•™ìŠµ ë°©ë²• | LoRA (Low-Rank Adaptation) |
| ë°ì´í„°ì…‹ | Alpaca (1000 ìƒ˜í”Œ) |
| Trainable Parameters | 4.3M (0.24%) |
| ì˜ˆìƒ í•™ìŠµ ì‹œê°„ | ì•½ 2ë¶„ |

### ìŠ¤í† ë¦¬ì§€ ì„¤ì •

ì´ ê°€ì´ë“œëŠ” FSxê°€ ì—†ëŠ” í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤:
- ëª¨ë¸: HuggingFace Hubì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
- ë°ì´í„°: HuggingFace Datasetsì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
- ì¶œë ¥: emptyDir (Pod ì¢…ë£Œ ì‹œ ì‚­ì œ)

> **ì£¼ì˜**: í•™ìŠµëœ ëª¨ë¸ì„ ì˜êµ¬ ì €ì¥í•˜ë ¤ë©´ FSx for Lustre ë˜ëŠ” S3ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### ì°¸ê³  ë¬¸ì„œ

- [Kubeflow Training Operator](https://github.com/kubeflow/training-operator)
- [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)
- [DeepSeek-R1 ëª¨ë¸](https://huggingface.co/deepseek-ai)
- [SageMaker HyperPod ë¬¸ì„œ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html)

---

## ğŸ”„ ê³ ê¸‰ ê¸°ëŠ¥: Auto Restart ë° ë³µì›ë ¥ (Resiliency)

ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµì—ì„œëŠ” í•˜ë“œì›¨ì–´ ì¥ì• , ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë“±ìœ¼ë¡œ ì¸í•œ í•™ìŠµ ì¤‘ë‹¨ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HyperPod EKS í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë³µì›ë ¥ ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1. PyTorchJobì˜ restartPolicy ì„¤ì •

`template/pytorchjob_finetuning.yaml`ì—ì„œ `restartPolicy`ë¥¼ ì„¤ì •í•˜ì—¬ Pod ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œì‘ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
spec:
  pytorchReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure    # ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œì‘
```

#### restartPolicy ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ì‚¬ìš© ì‹œê¸° |
|------|------|----------|
| `OnFailure` | Pod ì‹¤íŒ¨ ì‹œì—ë§Œ ì¬ì‹œì‘ | ì¼ë°˜ì ì¸ í•™ìŠµ ì‘ì—… (ê¶Œì¥) |
| `Always` | ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ í•­ìƒ ì¬ì‹œì‘ | ì§€ì†ì ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì„œë¹„ìŠ¤ |
| `Never` | ì¬ì‹œì‘í•˜ì§€ ì•ŠìŒ | ë””ë²„ê¹… ë˜ëŠ” ì¼íšŒì„± ì‘ì—… |
| `ExitCode` | íŠ¹ì • exit codeì— ë”°ë¼ ì¬ì‹œì‘ ê²°ì • | ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš° |

### 2. Checkpoint ê¸°ë°˜ í•™ìŠµ ì¬ê°œ

í•™ìŠµ ì¤‘ë‹¨ í›„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë ¤ë©´ **Checkpoint**ë¥¼ ì €ì¥í•˜ê³  ì¬ê°œí•´ì•¼ í•©ë‹ˆë‹¤.

#### YAML ìˆ˜ì •: ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ ë§ˆìš´íŠ¸

Checkpointë¥¼ ì €ì¥í•˜ë ¤ë©´ `emptyDir` ëŒ€ì‹  **FSx for Lustre** ë˜ëŠ” **EBS**ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```yaml
spec:
  pytorchReplicaSpecs:
    Worker:
      template:
        spec:
          volumes:
            - name: checkpoint-storage
              persistentVolumeClaim:
                claimName: fsx-claim
          containers:
            - name: pytorch
              volumeMounts:
                - name: checkpoint-storage
                  mountPath: /checkpoint
```

#### í•™ìŠµ ì½”ë“œ ìˆ˜ì •: Checkpoint ì €ì¥/ë¡œë“œ

```python
training_args = TrainingArguments(
    output_dir="/checkpoint/deepseek-finetuning",
    save_steps=100,              # 100 stepë§ˆë‹¤ checkpoint ì €ì¥
    save_total_limit=3,          # ìµœê·¼ 3ê°œ checkpointë§Œ ìœ ì§€
    resume_from_checkpoint=True, # ê¸°ì¡´ checkpointì—ì„œ ì¬ê°œ
)

trainer.train(resume_from_checkpoint=True)
```

### 3. HyperPodì˜ ìë™ ë…¸ë“œ ë³µêµ¬

SageMaker HyperPodëŠ” í´ëŸ¬ìŠ¤í„° ë ˆë²¨ì—ì„œ ìë™ ë³µêµ¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- **Deep Health Checks**: GPU/Trainium ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ ì‹¬ì¸µ í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰
- **ìë™ ë…¸ë“œ êµì²´**: í•˜ë“œì›¨ì–´ ì¥ì•  ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ë…¸ë“œ êµì²´

```bash
# Health Monitoring Agent í™•ì¸
kubectl get pods -n aws-hyperpod | grep health-monitoring
```

### 4. ë¶„ì‚° í•™ìŠµì—ì„œì˜ Elastic Training

ëŒ€ê·œëª¨ í•™ìŠµì—ì„œ ì¼ë¶€ ì›Œì»¤ê°€ ì‹¤íŒ¨í•´ë„ í•™ìŠµì„ ê³„ì†í•  ìˆ˜ ìˆëŠ” Elastic Training:

```yaml
spec:
  elasticPolicy:
    minReplicas: 1        # ìµœì†Œ ì›Œì»¤ ìˆ˜
    maxReplicas: 4        # ìµœëŒ€ ì›Œì»¤ ìˆ˜
    rdzvBackend: c10d     # PyTorch rendezvous backend
    maxRestarts: 3        # ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜
  pytorchReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
```

### 5. ë³µì›ë ¥ ê´€ë ¨ ëª¨ë‹ˆí„°ë§

```bash
# PyTorchJob ì´ë²¤íŠ¸ í™•ì¸
kubectl describe pytorchjob deepseek-finetuning | grep -A20 "Events:"

# Pod ì¬ì‹œì‘ íšŸìˆ˜ í™•ì¸
kubectl get pods -l training.kubeflow.org/job-name=deepseek-finetuning \
  -o custom-columns="NAME:.metadata.name,RESTARTS:.status.containerStatuses[0].restartCount"

# HyperPod ë…¸ë“œ í—¬ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get nodes -o custom-columns="NAME:.metadata.name,STATUS:.status.conditions[-1].type,READY:.status.conditions[-1].status"
```

### ë³µì›ë ¥ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | ì„¤ì • | í™•ì¸ |
|------|------|------|
| restartPolicy | `OnFailure` ì„¤ì • | âœ… |
| Checkpoint | ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ì— ì €ì¥ | âœ… |
| resume_from_checkpoint | í•™ìŠµ ì½”ë“œì— ì¶”ê°€ | âœ… |
| elasticPolicy | í•„ìš”ì‹œ ì„¤ì • | â¬œ |
| Health Monitoring | HyperPod ê¸°ë³¸ ì œê³µ | âœ… |
