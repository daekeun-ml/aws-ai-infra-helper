# HyperPod Inference Endpoint ë°°í¬ ë° í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (ìë™í™” ìŠ¤í¬ë¦½íŠ¸)

### 1. í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ ì„¤ì • (../../setup/1.create-config.sh ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” env_varì˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.)
```bash
./1.grant_eks_access.sh
```

### 2. ë°°í¬ ë°©ë²• ì„ íƒ

#### FSx ê¸°ë°˜ ë°°í¬ (AWS ê³„ì •)
```bash
# FSx í™˜ê²½ ì¤€ë¹„
./2.prepare_fsx_inference.sh

# FSxë¡œ ëª¨ë¸ ë³µì‚¬
kubectl apply -f copy_to_fsx_lustre.yaml

# ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
kubectl apply -f deploy_fsx_lustre_inference_operator.yaml
```

#### S3 ê¸°ë°˜ ë°°í¬ (AWS ê³„ì •)
```bash
# S3 í™˜ê²½ ì¤€ë¹„
./3.copy_to_s3.sh
./4.fix_s3_csi_credentials.sh
./5a.prepare_s3_inference.sh

# ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
kubectl apply -f deploy_S3_inference_operator.yaml
```

#### S3 ê¸°ë°˜ ë°°í¬ (AWS ì›Œí¬ìƒµ ì„ì‹œ ê³„ì •)
```bash
# S3 í™˜ê²½ ì¤€ë¹„
./3.copy_to_s3.sh
./5b.prepare_s3_direct_deploy.sh

# ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
kubectl apply -f deploy_S3_direct.yaml
```

### âš ï¸ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë¬¸ì œ í•´ê²°

`ml.g5.2xlarge` ë“± ì‘ì€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë…¸ë“œì— Podê°€ ë§ì•„ì„œ ë°°í¬ê°€ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°:

```bash
kubectl get pods -w

# NAME                          READY   STATUS    RESTARTS   AGE
# deepseek15b-59586756d-h7vsx   0/1     Pending   0          30s
```

```bash
# ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./fix_deployment_issues.sh

# ê¸°ì¡´ ë°°í¬ ì‚­ì œ í›„ ì¬ë°°í¬
kubectl delete deployment deepseek15b
kubectl apply -f deploy_S3_direct.yaml
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- Kueue/KEDA ë“± ë¶ˆí•„ìš”í•œ ì‹œìŠ¤í…œ Pod ì •ë¦¬
- ì™„ë£Œëœ Job Pod ì‚­ì œ
- PVC ë°”ì¸ë”© ë¬¸ì œ í•´ê²°
- Webhook ì„¤ì • ì œê±°

ì‹¤í–‰ í›„ ë‹¤ì‹œ ë°°í¬í•˜ì„¸ìš”.

## ğŸ“Š í…ŒìŠ¤íŠ¸ 

### AWS ì›Œí¬ìƒµ ì„ì‹œ ê³„ì •

```bash
# Pod ìƒíƒœ í™•ì¸
kubectl get pods -w

# ë¡œê·¸ í™•ì¸ (ëª¨ë¸ ë¡œë”© ì§„í–‰ ìƒí™©)
kubectl logs -l app=deepseek15b -f

# Service í™•ì¸
kubectl get svc deepseek15b

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
kubectl exec -it deployment/deepseek15b -- curl -X POST http://localhost:8080/invocations \
  -H 'Content-Type: application/json' \
  -d '{"inputs": "Explain machine learning in simple terms.", "parameters": {"max_new_tokens": 200, "temperature": 0.7, "repetition_penalty": 1.5}}'

# í…ŒìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ìš© Pod ë„ìš°ê³  ì‹¤í–‰)
kubectl run test-curl --rm -i --restart=Never --image=curlimages/curl -- \
  curl -X POST http://deepseek15b:8080/invocations \
  -H 'Content-Type: application/json' \
  -d '{"inputs": "Explain machine learning in simple terms.", "parameters": {"max_new_tokens": 200, "temperature": 0.7, "repetition_penalty": 1.5}}'
```

### AWS ê³„ì •
ë°°í¬ ì™„ë£Œ í›„ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (invoke.pyì—ì„œ ENDPOINT_NAME ìˆ˜ì • í•„ìš”)
python invoke.py
```

> **ì°¸ê³ **: `invoke.py` íŒŒì¼ì—ì„œ `ENDPOINT_NAME`ì„ ë°°í¬í•œ ì—”ë“œí¬ì¸íŠ¸ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
> - FSx ë°°í¬: `'deepseek15b-fsx'`
> - S3 ë°°í¬: `'deepseek15b'` (ë˜ëŠ” ì‚¬ìš©ì ì •ì˜ ì´ë¦„)

---

## ğŸ“– ìƒì„¸ ê°€ì´ë“œ (ìˆ˜ë™ Step-by-Step)

ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  ê° ë‹¨ê³„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´í•´í•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ê°€ì´ë“œë¥¼ ë”°ë¥´ì„¸ìš”.

### ì‚¬ì „ ì¤€ë¹„

### 0. EKS í´ëŸ¬ìŠ¤í„° ìƒì„±

HyperPodì—ì„œ EKS ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ìƒì„±í•˜ëŠ” [ê°€ì´ë“œë¼ì¸](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-console-ui-create-cluster.html
)ì„ ì°¸ê³ í•˜ì—¬ EKS í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### 1. EKS í´ëŸ¬ìŠ¤í„° ì ‘ì† ì„¤ì •

```bash
# HyperPod EKS í´ëŸ¬ìŠ¤í„°ì— kubeconfig ì„¤ì • (Consoleì—ì„œ í´ëŸ¬ìŠ¤í„° í´ë¦­ í›„ Orchestrator í•­ëª©ì—ì„œ ì´ë¦„ í™•ì¸ ê°€ëŠ¥)
aws eks update-kubeconfig --name "YOUR_EKS_CLUSTER_NAME" --region us-west-2

# í´ëŸ¬ìŠ¤í„° ì—°ê²° í™•ì¸
kubectl get nodes
```

### 2. PVC ìƒíƒœ í™•ì¸

```bash
kubectl get pvc
```

ì¶œë ¥ ì˜ˆì‹œ:
```
NAME        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS
fsx-claim   Bound    fsx-pv   1200Gi     RWX            fsx-sc
```

---

## ë°©ë²• 1: FSX Lustre ê¸°ë°˜ Endpoint ë°°í¬

### Step 1: ëª¨ë¸ì„ FSXë¡œ ë³µì‚¬

**copy.yaml íŒŒì¼ ìƒì„±:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: copy-model-to-fsx
spec:
  template:
    spec:
      containers:
        - name: aws-cli
          image: amazon/aws-cli:latest
          command: ["/bin/bash"]
          args:
            - -c
            - |
              aws s3 sync s3://jumpstart-cache-prod-us-east-2/deepseek-llm/deepseek-llm-r1-distill-qwen-1-5b/artifacts/inference-prepack/v2.0.0 /fsx/deepseek15b
          volumeMounts:
            - name: fsx-storage
              mountPath: /fsx
          env:
            - name: AWS_DEFAULT_REGION
              value: "us-west-2"
            - name: AWS_REGION
              value: "us-west-2"
            - name: AWS_ACCESS_KEY_ID
              value: "<YOUR_ACCESS_KEY_ID>"
            - name: AWS_SECRET_ACCESS_KEY
              value: "<YOUR_SECRET_ACCESS_KEY>"
            - name: AWS_SESSION_TOKEN
              value: "<YOUR_SESSION_TOKEN>"  # ì„ì‹œ ìê²© ì¦ëª… ì‚¬ìš© ì‹œ
      volumes:
        - name: fsx-storage
          persistentVolumeClaim:
            claimName: fsx-claim
      restartPolicy: Never
  backoffLimit: 3
```

**Job ì‹¤í–‰:**
```bash
kubectl apply -f copy_to_fsx_lustre.yaml
```

**ë³µì‚¬ ìƒíƒœ í™•ì¸:**
```bash
# Job ìƒíƒœ í™•ì¸
kubectl get jobs

# Pod ë¡œê·¸ í™•ì¸ (ë³µì‚¬ ì§„í–‰ë¥ )
kubectl logs -f job/copy-model-to-fsx
```

### Step 2: FSX File System ID í™•ì¸

```bash
kubectl get pv fsx-pv -o yaml | grep -A5 "csi:"
```

ì¶œë ¥ ì˜ˆì‹œ:
```yaml
csi:
  driver: fsx.csi.aws.com
  volumeAttributes:
    dnsname: fs-09d6a597bc983fe33.fsx.us-west-2.amazonaws.com
    mountname: e3pfzb4v
  volumeHandle: fs-09d6a597bc983fe33
```

### Step 3: FSX Endpoint ë°°í¬

**deploy_fsx_lustre_inference_operator.yaml íŒŒì¼ì—ì„œ fileSystemId ìˆ˜ì •:**
```yaml
apiVersion: inference.sagemaker.aws.amazon.com/v1alpha1
kind: InferenceEndpointConfig
metadata:
  name: deepseek15b-fsx
  namespace: default
spec:
  endpointName: deepseek15b-fsx
  instanceType: ml.g5.8xlarge
  invocationEndpoint: invocations
  modelName: deepseek15b
  modelSourceConfig:
    fsxStorage:
      fileSystemId: fs-09d6a597bc983fe33  # ìœ„ì—ì„œ í™•ì¸í•œ FSX IDë¡œ ë³€ê²½
    modelLocation: deepseek15b
    modelSourceType: fsx
  worker:
    environmentVariables:
    - name: HF_MODEL_ID
      value: /opt/ml/model
    - name: SAGEMAKER_PROGRAM
      value: inference.py
    - name: SAGEMAKER_SUBMIT_DIRECTORY
      value: /opt/ml/model/code
    - name: MODEL_CACHE_ROOT
      value: /opt/ml/model
    - name: SAGEMAKER_ENV
      value: '1'
    image: 763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.4.0-tgi2.3.1-gpu-py311-cu124-ubuntu22.04-v2.0
    modelInvocationPort:
      containerPort: 8080
      name: http
    modelVolumeMount:
      mountPath: /opt/ml/model
      name: model-weights
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        cpu: 30000m
        memory: 100Gi
        nvidia.com/gpu: 1
```

**Endpoint ë°°í¬:**
```bash
kubectl apply -f deploy_fsx_lustre_inference_operator.yaml
```

**ë°°í¬ ìƒíƒœ í™•ì¸:**
```bash
# Pod ìƒíƒœ í™•ì¸
kubectl get pods

# ìƒì„¸ ì´ë²¤íŠ¸ í™•ì¸
kubectl describe pod -l app=deepseek15b-fsx
```

---

## ë°©ë²• 2: S3 ê¸°ë°˜ Endpoint ë°°í¬

### Step 1: S3 ë²„í‚· ìƒì„± ë° ëª¨ë¸ ì—…ë¡œë“œ

```bash
# S3 ë²„í‚· ìƒì„± (í´ëŸ¬ìŠ¤í„°ì™€ ê°™ì€ ë¦¬ì „)
aws s3 mb s3://deepseek-qwen-1-5b-us-west-2 --region us-west-2

# ëª¨ë¸ ë³µì‚¬
aws s3 sync s3://jumpstart-cache-prod-us-east-2/deepseek-llm/deepseek-llm-r1-distill-qwen-1-5b/artifacts/inference-prepack/v2.0.0 \
  s3://deepseek-qwen-1-5b-us-west-2/deepseek15b/ --region us-west-2
```

### Step 2: S3 Endpoint ë°°í¬

**deploy_S3_inference_operator.yaml:**
```yaml
apiVersion: inference.sagemaker.aws.amazon.com/v1alpha1
kind: InferenceEndpointConfig
metadata:
  name: deepseek15b
  namespace: default
spec:
  modelName: deepseek15b
  endpointName: deepseek15b
  instanceType: ml.g5.8xlarge
  invocationEndpoint: invocations
  modelSourceConfig:
    modelSourceType: s3
    s3Storage:
      bucketName: deepseek-qwen-1-5b-us-west-2  # ìƒì„±í•œ ë²„í‚· ì´ë¦„
      region: us-west-2                         # ë²„í‚· ë¦¬ì „
    modelLocation: deepseek15b
    prefetchEnabled: true
  worker:
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
        cpu: 25600m
        memory: 102Gi
    image: 763104351884.dkr.ecr.us-east-2.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu124
    modelInvocationPort:
      containerPort: 8080
      name: http
    modelVolumeMount:
      name: model-weights
      mountPath: /opt/ml/model
    environmentVariables:
      - name: OPTION_ROLLING_BATCH
        value: "vllm"
      - name: SERVING_CHUNKED_READ_TIMEOUT
        value: "480"
      - name: DJL_OFFLINE
        value: "true"
      - name: NUM_SHARD
        value: "1"
      - name: SAGEMAKER_PROGRAM
        value: "inference.py"
      - name: SAGEMAKER_SUBMIT_DIRECTORY
        value: "/opt/ml/model/code"
      - name: MODEL_CACHE_ROOT
        value: "/opt/ml/model"
      - name: SAGEMAKER_MODEL_SERVER_WORKERS
        value: "1"
      - name: SAGEMAKER_MODEL_SERVER_TIMEOUT
        value: "3600"
      - name: OPTION_TRUST_REMOTE_CODE
        value: "true"
      - name: OPTION_ENABLE_REASONING
        value: "true"
      - name: OPTION_REASONING_PARSER
        value: "deepseek_r1"
      - name: SAGEMAKER_CONTAINER_LOG_LEVEL
        value: "20"
      - name: SAGEMAKER_ENV
        value: "1"
```

**Endpoint ë°°í¬:**
```bash
kubectl apply -f deploy_S3_inference_operator.yaml
```

**ë°°í¬ ìƒíƒœ í™•ì¸:**
```bash
kubectl get pods
kubectl get svc
```

---

## Endpoint í…ŒìŠ¤íŠ¸

### Step 1: í…ŒìŠ¤íŠ¸ìš© Pod ìƒì„±

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-endpoint
spec:
  containers:
  - name: test
    image: python:3.11-slim
    command: ["sleep", "3600"]
  restartPolicy: Never
EOF
```

### Step 2: Pod ìƒíƒœ í™•ì¸

```bash
kubectl get pod test-endpoint
```

### Step 3: requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
kubectl exec test-endpoint -- pip install requests -q
```

### Step 4: Endpoint ì •ë³´ í™•ì¸

```bash
kubectl get endpoints
```

ì¶œë ¥ ì˜ˆì‹œ:
```
NAME                              ENDPOINTS
deepseek15b-fsx-routing-service   10.1.112.202:8081
deepseek15b-routing-service       10.1.111.162:8081
```

### Step 5: FSX Endpoint í…ŒìŠ¤íŠ¸

> **Note:** Service DNS ì´ë¦„ ëŒ€ì‹  Endpoint IPë¥¼ ì§ì ‘ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. Step 4ì—ì„œ í™•ì¸í•œ IPë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```bash
# Endpoint IP í™•ì¸ (ì˜ˆ: 10.1.112.202:8081)
kubectl get endpoints deepseek15b-fsx-routing-service

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (IPë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½)
kubectl exec test-endpoint -- python3 -c '
import requests
import json

response = requests.post(
    "http://10.1.112.202:8081/invocations",  # ì‹¤ì œ Endpoint IPë¡œ ë³€ê²½
    headers={"Content-Type": "application/json"},
    json={"inputs": "Hi, what can you help me with?"},
    timeout=120
)
print(f"Status: {response.status_code}")
print(f"Response: {response.text}")
'
```

### Step 6: S3 Endpoint í…ŒìŠ¤íŠ¸

```bash
# Endpoint IP í™•ì¸ (ì˜ˆ: 10.1.111.162:8081)
kubectl get endpoints deepseek15b-routing-service

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (IPë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½)
kubectl exec test-endpoint -- python3 -c '
import requests
import json

response = requests.post(
    "http://10.1.111.162:8081/invocations",  # ì‹¤ì œ Endpoint IPë¡œ ë³€ê²½
    headers={"Content-Type": "application/json"},
    json={"inputs": "Hi, what can you help me with?"},
    timeout=120
)
print(f"Status: {response.status_code}")
print(f"Response: {response.text}")
'
```

### Step 7: í…ŒìŠ¤íŠ¸ Pod ì •ë¦¬

```bash
kubectl delete pod test-endpoint
```

---

## ë¦¬ì†ŒìŠ¤ ì •ë¦¬

### Endpoint ì‚­ì œ

```bash
# FSX Endpoint ì‚­ì œ
kubectl delete inferenceendpointconfig deepseek15b-fsx

# S3 Endpoint ì‚­ì œ
kubectl delete inferenceendpointconfig deepseek15b
```

### ë³µì‚¬ Job ì‚­ì œ

```bash
kubectl delete job copy-model-to-fsx
```

### S3 ë²„í‚· ì‚­ì œ (ì„ íƒì‚¬í•­)

```bash
aws s3 rb s3://deepseek-qwen-1-5b-us-west-2 --force --region us-west-2
```

---

## ìœ ìš©í•œ ëª…ë ¹ì–´

```bash
# ëª¨ë“  ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods,svc,jobs,inferenceendpointconfig

# Pod ë¡œê·¸ í™•ì¸
kubectl logs <pod-name>

# Pod ìƒì„¸ ì •ë³´ (ì´ë²¤íŠ¸ í¬í•¨)
kubectl describe pod <pod-name>

# InferenceEndpointConfig ìƒì„¸ ì •ë³´
kubectl describe inferenceendpointconfig <name>
```
