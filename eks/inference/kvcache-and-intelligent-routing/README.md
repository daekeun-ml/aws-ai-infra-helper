# SageMaker HyperPod Inference: KV Cache & Intelligent Routing ë²¤ì¹˜ë§ˆí¬

AWS SageMaker HyperPodì˜ Managed Tiered KV Cacheì™€ Intelligent Routing ê¸°ëŠ¥ì„ ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì¢…í•© ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.

## ğŸ“š ì°¸ê³  ìë£Œ

### AWS ê³µì‹ ë¬¸ì„œ
- [Managed Tiered KV Cache and Intelligent Routing ë¸”ë¡œê·¸](https://aws.amazon.com/blogs/machine-learning/managed-tiered-kv-cache-and-intelligent-routing-for-amazon-sagemaker-hyperpod/)
- [HyperPod Model Deployment ê°€ì´ë“œ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-model-deployment.html)
- [HyperPod Cluster Setup](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-model-deployment-setup.html)

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### 1. Managed Tiered KV Cache
- **L1 Cache**: CPU ë©”ëª¨ë¦¬ (ë¡œì»¬, ë¹ ë¥¸ ì ‘ê·¼)
- **L2 Cache**: í´ëŸ¬ìŠ¤í„° ì „ì²´ ê³µìœ  ìºì‹œ
  - **tieredstorage** (ê¶Œì¥): AWS ìµœì í™”, í…Œë¼ë°”ì´íŠ¸ ê·œëª¨, GPU-aware, zero-copy
  - **redis**: ì†Œê·œëª¨ ì›Œí¬ë¡œë“œìš©

### 2. Intelligent Routing ì „ëµ

| ì „ëµ | ì„¤ëª… | ìµœì  ì‚¬ìš© ì‚¬ë¡€ |
|------|------|----------------|
| **prefix-aware** (ê¸°ë³¸) | í”„ë¦¬í”½ìŠ¤ íŠ¸ë¦¬ë¡œ ìºì‹œ ìœ„ì¹˜ ì¶”ì  | ë©€í‹°í„´ ëŒ€í™”, ê³ ê° ì„œë¹„ìŠ¤ ë´‡, ê³µí†µ í…œí”Œë¦¿ |
| **kv-aware** | ì¤‘ì•™ ì»¨íŠ¸ë¡¤ëŸ¬ë¡œ ì‹¤ì‹œê°„ ìºì‹œ ê´€ë¦¬ | ê¸´ ëŒ€í™”, ë¬¸ì„œ ì²˜ë¦¬, í™•ì¥ ì½”ë”© ì„¸ì…˜ |
| **round-robin** | ê· ë“± ë¶„ì‚° | ë°°ì¹˜ ì¶”ë¡ , ìƒíƒœ ì—†ëŠ” API, ë¡œë“œ í…ŒìŠ¤íŠ¸ |

## ğŸ“Š AWS ê³µì‹ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

### ì¤‘ê°„ ì»¨í…ìŠ¤íŠ¸ (8K í† í°)
- TTFT P90: **40% ê°ì†Œ**
- TTFT P50: **72% ê°ì†Œ**
- Throughput: **24% ì¦ê°€**
- Cost: **21% ì ˆê°**

### ê¸´ ì»¨í…ìŠ¤íŠ¸ (64K í† í°)
- TTFT P90: **35% ê°ì†Œ**
- TTFT P50: **94% ê°ì†Œ**
- Throughput: **38% ì¦ê°€**
- Cost: **28% ì ˆê°**

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ì¤€ë¹„ì‚¬í•­
1. SageMaker HyperPod í´ëŸ¬ìŠ¤í„° (EKS ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)
2. Inference Operator ì„¤ì¹˜ ì™„ë£Œ
3. AWS CLI ë° kubectl ì„¤ì •

### ë‹¨ê³„ë³„ ê°€ì´ë“œ

#### 1. ë„êµ¬ ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
```bash
# kubectl, eksctl, helm ì„¤ì¹˜
./install_tools.sh
```

#### 2. S3 ë²„í‚· ìƒì„± ë° ëª¨ë¸ ë³µì‚¬
```bash
# S3 ë²„í‚· ìƒì„±, ë²„í‚· ì •ì±… ì„¤ì •, ëª¨ë¸ ë³µì‚¬
./1.copy_to_s3.sh

# S3_BUCKET í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì¶œë ¥ëœ ë²„í‚· ì´ë¦„ ì‚¬ìš©)
export S3_BUCKET=hyperpod-inference-xxxxx-us-west-2
```

#### 3. S3 CSI Driver ì„¤ì •
```bash
# S3 CSI Driver IAM ê¶Œí•œ ì„¤ì • (í•„ìˆ˜)
./2.setup_s3_csi.sh

# 1-2ë¶„ ëŒ€ê¸° í›„ ìƒíƒœ í™•ì¸
kubectl get pods -n kube-system | grep s3-csi
```

#### 4. ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
```bash
# inference_endpoint_config.yaml ìƒì„± ë° ë°°í¬
./3.prepare.sh

# ë°°í¬ ì™„ë£Œê¹Œì§€ ì•½ 5-10ë¶„ ì†Œìš”
# - S3ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# - ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ pull
# - ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”
```

#### 5. ìƒíƒœ í™•ì¸
```bash
# ì—”ë“œí¬ì¸íŠ¸ ë° Pod ìƒíƒœ í™•ì¸
./4.check_status.sh

# ë˜ëŠ” ìˆ˜ë™ í™•ì¸
kubectl get inferenceendpointconfig demo -n default
kubectl get pods -n default
kubectl describe inferenceendpointconfig demo -n default

# Pod ìƒíƒœê°€ "3/3 Running"ì´ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì•½ 5-10ë¶„)
watch kubectl get pods -n default
```

#### 6. ë°°í¬ ì™„ë£Œ í™•ì¸ ë° í…ŒìŠ¤íŠ¸
```bash
# Podê°€ ëª¨ë‘ Running ìƒíƒœê°€ ë˜ë©´ í…ŒìŠ¤íŠ¸
# Worker Pod: 3/3 Running í™•ì¸ í•„ìˆ˜

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
python invoke.py

# ì¢…í•© ë²¤ì¹˜ë§ˆí¬ (ë™ì‹œ ìš”ì²­ 20ê±´)
python benchmark.py
```

**ë°°í¬ ì‹œê°„:**
- ì´ˆê¸° ë°°í¬: ì•½ 5-10ë¶„ ì†Œìš”
  - S3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: 2-3ë¶„
  - ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”: 1-2ë¶„
  - ëª¨ë¸ ë¡œë”©: 2-5ë¶„
- ì¬ë°°í¬: ì•½ 3-5ë¶„ (ìºì‹œëœ ì´ë¯¸ì§€ ì‚¬ìš©)

### ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • ì˜ˆì‹œ

`inference_endpoint_config.yaml`:
```yaml
apiVersion: inference.sagemaker.aws.amazon.com/v1
kind: InferenceEndpointConfig
metadata:
  name: demo
  namespace: default
spec:
  modelName: Llama-3.1-8B-Instruct
  instanceType: ml.g5.24xlarge
  replicas: 1
  invocationEndpoint: v1/chat/completions
  
  # KV Cache ì„¤ì •
  kvCacheSpec:
    enableL1Cache: true
    enableL2Cache: true
    l2CacheSpec:
      l2CacheBackend: "tieredstorage"  # ê¶Œì¥
  
  # Intelligent Routing ì„¤ì •
  intelligentRoutingSpec:
    enabled: true
    routingStrategy: prefixaware  # prefix-aware, kv-aware, round-robin
  
  modelSourceConfig:
    modelSourceType: s3
    s3Storage:
      bucketName: my-model-bucket
      region: us-west-2
    modelLocation: models/Llama-3.1-8B-Instruct
  
  worker:
    resources:
      limits:
        nvidia.com/gpu: "4"
      requests:
        cpu: "6"
        memory: 30Gi
        nvidia.com/gpu: "4"
    image: public.ecr.aws/deep-learning-containers/vllm:0.11.1-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    args:
      - "--model"
      - "/opt/ml/model"
      - "--max-model-len"
      - "20000"
      - "--tensor-parallel-size"
      - "4"
```

## ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • í•­ëª©

### 1. TTFT (Time To First Token)
- P50, P90, P95, P99 ë°±ë¶„ìœ„ìˆ˜
- Cold Cache vs Warm Cache ë¹„êµ

### 2. Throughput (TPS)
- Tokens Per Second
- Cold Cache vs Warm Cache ë¹„êµ

### 3. Cost Analysis
- Cost per 1K tokens
- Input/Output í† í°ë³„ ë¹„ìš© ê³„ì‚°

### 4. Prefix-aware Routing íš¨ê³¼
- ê°™ì€ Prefix ë°˜ë³µ vs ë‹¤ë¥¸ Prefix
- ìºì‹œ íˆíŠ¸ìœ¨ ì¸¡ì •

## ğŸ’¡ í•µì‹¬ ë°œê²¬

### KV Cache íš¨ê³¼
- **ì²« ìš”ì²­**: ìºì‹œ ë¯¸ìŠ¤ (ëŠë¦¼)
- **ì´í›„ ìš”ì²­**: ìºì‹œ íˆíŠ¸ (40-50% ë¹ ë¦„)
- **L2 Cache ê³µìœ **: ëª¨ë“  ì›Œì»¤ê°€ ìºì‹œ ê³µìœ 

### Prefix-aware Routing
- ê°™ì€ prefix â†’ ê°™ì€ ì›Œì»¤ â†’ KV Cache ì¬ì‚¬ìš©
- ë‹¤ë¥¸ prefix â†’ ë‹¤ë¥¸ ì›Œì»¤ â†’ ìºì‹œ ë¯¸ìŠ¤
- ë©€í‹°í„´ ëŒ€í™”, ë¬¸ì„œ Q&Aì— ìµœì 

## ğŸ“ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### 1. ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ
```python
# ê¸´ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
DOCUMENT = "... ë§¤ìš° ê¸´ ë¬¸ì„œ ë‚´ìš© ..."

# ì—¬ëŸ¬ ì§ˆë¬¸ (ê°™ì€ ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©)
for question in questions:
    response = invoke_endpoint(
        messages=[{"role": "user", "content": f"{DOCUMENT}\n\n{question}"}]
    )
```

### 2. ì½”ë“œ ë¦¬ë·° ì–´ì‹œìŠ¤í„´íŠ¸
```python
# ê¸´ ì½”ë“œë² ì´ìŠ¤ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ
CODE = "... ì „ì²´ ì½”ë“œ ..."

# ì—¬ëŸ¬ ë¦¬ë·° ì§ˆë¬¸
for review_question in review_questions:
    response = invoke_endpoint(
        messages=[{"role": "user", "content": f"{CODE}\n\n{review_question}"}]
    )
```

### 3. ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜
```python
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ëˆ„ì 
conversation_history = []

for user_message in user_messages:
    conversation_history.append({"role": "user", "content": user_message})
    
    response = invoke_endpoint(messages=conversation_history)
    
    conversation_history.append({"role": "assistant", "content": response})
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë°°í¬ ìƒíƒœ í™•ì¸
```bash
# ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§
watch kubectl get pods -n default

# ìƒì„¸ ë¡œê·¸ í™•ì¸
kubectl logs -l app=demo -n default -f

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events -n default --sort-by='.lastTimestamp'
```

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**1. S3 ê¶Œí•œ ì˜¤ë¥˜**
- `2.setup_s3_csi.sh`ê°€ ìë™ìœ¼ë¡œ ê¶Œí•œ ì„¤ì •
- ìˆ˜ë™ í™•ì¸: IAM ì—­í• ì— S3 ì ‘ê·¼ ê¶Œí•œ í™•ì¸

**2. Podê°€ ContainerCreating ìƒíƒœì—ì„œ ë©ˆì¶¤**
- S3 ë§ˆìš´íŠ¸ ë¬¸ì œì¼ ê°€ëŠ¥ì„±
- S3 CSI Driver pods í™•ì¸: `kubectl get pods -n kube-system | grep s3-csi`
- Mountpoint logs í™•ì¸: `kubectl logs -n mount-s3 <pod-name>`

**3. ëª¨ë¸ ë¡œë”© ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼**
- ì •ìƒ: ì²« ë°°í¬ëŠ” 5-10ë¶„ ì†Œìš”
- Worker logs í™•ì¸: `kubectl logs -l app=demo -n default`

### ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ
```bash
# ì „ì²´ ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ
./cleanup.sh

# S3 ë²„í‚·ë„ í•¨ê»˜ ì‚­ì œ
export S3_BUCKET=hyperpod-inference-xxxxx-us-east-2
./cleanup.sh

# ë˜ëŠ” ìˆ˜ë™ ì‚­ì œ
kubectl delete inferenceendpointconfig demo -n default
```

### Pod ì¬ì‹œì‘ (ìºì‹œ ì´ˆê¸°í™”)
```bash
# Podë§Œ ì‚­ì œ (ìë™ìœ¼ë¡œ ì¬ìƒì„±ë¨)
kubectl delete pod <pod-name> -n default
```

## ğŸ“ íŒŒì¼ êµ¬ì„±

```
.
â”œâ”€â”€ install_tools.sh                   # kubectl, eksctl, helm ì„¤ì¹˜
â”œâ”€â”€ 1.copy_to_s3.sh                    # S3 ë²„í‚· ìƒì„± ë° ëª¨ë¸ ë³µì‚¬
â”œâ”€â”€ 2.setup_s3_csi.sh                  # S3 CSI Driver ì„¤ì •
â”œâ”€â”€ 3.prepare.sh                       # ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
â”œâ”€â”€ 4.check_status.sh                  # ìƒíƒœ í™•ì¸
â”œâ”€â”€ cleanup.sh                         # ì—”ë“œí¬ì¸íŠ¸ ì‚­ì œ
â”œâ”€â”€ invoke.py                          # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ benchmark.py                       # ì¢…í•© ë²¤ì¹˜ë§ˆí¬
â””â”€â”€ README.md                          # README
```

## ğŸ”— ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [AWS SageMaker HyperPod ë¬¸ì„œ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html)
- [HyperPod Inference Operator](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/eks-blueprints/inference/inference-operator/)
- [HyperPod CLI & SDK](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-hyperpod-training-deploying-models.html)

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” AWS ìƒ˜í”Œ ì½”ë“œì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ PRì„ í™˜ì˜í•©ë‹ˆë‹¤!

---

**ì°¸ê³ **: ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì›Œí¬ë¡œë“œì™€ ì„¤ì •ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
