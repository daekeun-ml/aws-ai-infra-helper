# AWS AI Infra helper for SageMaker HyperPod and ParallelCluster

AWS SageMaker HyperPod ë° ParallelClusterë¥¼ ìœ„í•œ í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ë° ê°€ì´ë“œ ëª¨ìŒì…ë‹ˆë‹¤. HPC í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ ë° ì¶”ë¡ ì„ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ What's New

### v1.0.1 (2025-12-21)
- **FSDP2 ì§€ì› ì¶”ê°€**: PyTorch 2.5+ FSDP2 ê¸°ë°˜ ë¶„ì‚° í•™ìŠµ ì˜ˆì œ ë° ê°€ì´ë“œ
- **DeepSpeed í†µí•©**: DeepSpeed ZeRO ê¸°ë°˜ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ ìƒ˜í”Œ ì¶”ê°€
- **Qwen 3 0.6B í…ŒìŠ¤íŠ¸**: ìµœì‹  Qwen 3 0.6B ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡  ì˜ˆì œ (p4/p5 ì¸ìŠ¤í„´ìŠ¤ ê¶Œì¥)
- **ì„±ëŠ¥ ìµœì í™”**: ìµœì‹  GPU ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì— ìµœì í™”ëœ ì„¤ì • ë° ê°€ì´ë“œ

## ê°œìš”

ì´ ì €ì¥ì†ŒëŠ” ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤:

- **í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸**: HyperPod í´ëŸ¬ìŠ¤í„° ì—°ê²° ë° ì„¤ì • ë„êµ¬
- **ë¶„ì‚° í•™ìŠµ ì˜ˆì œ**: FSDP, Megatron-LM, TorchTitanì„ ì‚¬ìš©í•œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ
- **ê²€ì¦ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸**: í´ëŸ¬ìŠ¤í„° í™˜ê²½ ê²€ì¦ ë° í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
- **í•œêµ­ì–´ ê°€ì´ë“œ**: ê° í”„ë ˆì„ì›Œí¬ë³„ ìƒì„¸í•œ í•œêµ­ì–´ ë¬¸ì„œ

## ê¸°ìˆ  ìŠ¤íƒ

- **AWS ì„œë¹„ìŠ¤**: SageMaker HyperPod (w/ Slurm), AWS ParallelCluster
- **ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬**: PyTorch FSDP, Megatron-LM, TorchTitan
- **ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„**: Pyxis/Enroot (Slurm ì»¨í…Œì´ë„ˆ ì§€ì›)
- **ë„¤íŠ¸ì›Œí¬**: AWS EFA (Elastic Fabric Adapter)

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
aws-ai-infra-helper/
â”œâ”€â”€ scripts/              # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ hyperpod-connect.sh       # SSM ê¸°ë°˜ HyperPod ì—°ê²°
â”‚   â”œâ”€â”€ hyperpod-ssh.sh           # SSH ê¸°ë°˜ HyperPod ì—°ê²°
â”‚   â”œâ”€â”€ check-fsx.sh              # FSx for Lustre ê²€ì¦
â”‚   â”œâ”€â”€ check-munged.sh           # Slurm ì—°ê²° ê²€ì¦
â”‚   â”œâ”€â”€ check-pyxis-enroot.sh     # Pyxis/Enroot ì„¤ì¹˜ ê²€ì¦
â”‚   â”œâ”€â”€ install-pyxis-enroot.sh   # ì»¨í…Œì´ë„ˆ ì§€ì› ì„¤ì¹˜
â”‚   â”œâ”€â”€ install-nccl-efa.sh       # NCCL/EFA ì„¤ì¹˜
â”‚   â”œâ”€â”€ fix-cuda-version.sh       # CUDA ë²„ì „ í™•ì¸ ë° ìˆ˜ì •
â”‚   â””â”€â”€ generate-nccl-test.sh     # NCCL í…ŒìŠ¤íŠ¸ ìƒì„±
â”‚
â”œâ”€â”€ fsdp/                 # PyTorch FSDP ì˜ˆì œ
â”‚   â”œâ”€â”€ README.md                 # FSDP í•œêµ­ì–´ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ train-fsdp.sbatch         # ë©€í‹°ë…¸ë“œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train-fsdp-singlegpu.sbatch  # ë‹¨ì¼ GPU í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ train.py              # FSDP í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”œâ”€â”€ requirements.txt      # Python ì˜ì¡´ì„±
â”‚       â””â”€â”€ model_utils/          # ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ fsdp2/                # PyTorch FSDP2 ì˜ˆì œ (NEW)
â”‚   â”œâ”€â”€ README.md                 # FSDP2 í•œêµ­ì–´ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ train-fsdp2.sbatch        # ë©€í‹°ë…¸ë“œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train-fsdp2-singlenode.sh # ë‹¨ì¼ ë…¸ë“œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ train_fsdp2.py        # FSDP2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚       â””â”€â”€ model_utils/          # ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ deepspeed/            # DeepSpeed ì˜ˆì œ (NEW)
â”‚   â”œâ”€â”€ README.md                 # DeepSpeed í•œêµ­ì–´ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ train-qwen3-0-6b.sbatch   # Qwen 3 0.6B í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train-qwen3-0-6b-singlenode.sh  # ë‹¨ì¼ ë…¸ë“œ í•™ìŠµ
â”‚   â”œâ”€â”€ ds_config.json            # DeepSpeed ì„¤ì •
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ train_deepspeed.py    # DeepSpeed í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚       â””â”€â”€ model_utils/          # ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ megatron/             # Megatron-LM ì˜ˆì œ
â”‚   â”œâ”€â”€ megatron-lm-slurm-guide-ko.md  # Slurm ê°€ì´ë“œ
â”‚   â””â”€â”€ megatron-lm-eks-guide-ko.md    # EKS ê°€ì´ë“œ
â”‚
â”œâ”€â”€ torchtitan/           # TorchTitan ì˜ˆì œ
â”‚   â”œâ”€â”€ torchtitan-guide-ko.md    # TorchTitan í•œêµ­ì–´ ê°€ì´ë“œ
â”‚   â””â”€â”€ torchtitan-multinode.sbatch  # ë©€í‹°ë…¸ë“œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ observability/        # ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„± ë„êµ¬
â”‚   â”œâ”€â”€ install_observability.py  # í†µí•© ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ run-observability.sh      # ê´€ì°°ì„± ë„êµ¬ ì‹¤í–‰
â”‚   â”œâ”€â”€ stop_observability.py     # ê´€ì°°ì„± ë„êµ¬ ì¤‘ì§€
â”‚   â”œâ”€â”€ install_node_exporter.sh  # Node Exporter ì„¤ì¹˜
â”‚   â”œâ”€â”€ install_dcgm_exporter.sh  # DCGM Exporter ì„¤ì¹˜
â”‚   â”œâ”€â”€ install_efa_exporter.sh   # EFA Exporter ì„¤ì¹˜
â”‚   â”œâ”€â”€ install_slurm_exporter.sh # Slurm Exporter ì„¤ì¹˜
â”‚   â”œâ”€â”€ install_otel_collector.sh # OpenTelemetry Collector ì„¤ì¹˜
â”‚   â”œâ”€â”€ otel_config/              # OTel ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ dcgm_metrics_config/      # DCGM ë©”íŠ¸ë¦­ ì„¤ì •
â”‚
â””â”€â”€ eks/                  # EKS ê´€ë ¨ ë„êµ¬ ë° ê°€ì´ë“œ
    â”œâ”€â”€ training/         # EKS í•™ìŠµ í´ëŸ¬ìŠ¤í„° ì„¤ì •
    â”‚   â”œâ”€â”€ README.md             # EKS í•™ìŠµ ê°€ì´ë“œ
    â”‚   â”œâ”€â”€ 1.create-config.sh    # í™˜ê²½ ì„¤ì • ìƒì„±
    â”‚   â”œâ”€â”€ 2.setup-eks-access.sh # EKS ì ‘ê·¼ ê¶Œí•œ ì„¤ì •
    â”‚   â”œâ”€â”€ 3.validate-cluster.sh # í´ëŸ¬ìŠ¤í„° ê²€ì¦
    â”‚   â””â”€â”€ check-nodegroup.sh    # NodeGroup ì •ë³´ í™•ì¸
    â”‚
    â””â”€â”€ inference/        # EKS ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸
        â”œâ”€â”€ README.md             # ì¶”ë¡  ë°°í¬ ê°€ì´ë“œ
        â”œâ”€â”€ deploy_S3_inference_operator.yaml
        â”œâ”€â”€ deploy_fsx_lustre_inference_operator.yaml
        â””â”€â”€ copy_to_fsx_lustre.yaml
```

## ë¹ ë¥¸ ì‹œì‘

### 1. í´ëŸ¬ìŠ¤í„° ì—°ê²°

#### SSMì„ í†µí•œ ì—°ê²° (ê¶Œì¥)

```bash
# í—¤ë“œ ë…¸ë“œ ì—°ê²°
./scripts/hyperpod-connect.sh

# íŠ¹ì • í´ëŸ¬ìŠ¤í„° ì§€ì •
./scripts/hyperpod-connect.sh --cluster-name my-cluster
```

#### SSHë¥¼ í†µí•œ ì—°ê²°

```bash
./scripts/hyperpod-ssh.sh --cluster-name my-cluster
```

### 2. í™˜ê²½ ê²€ì¦

```bash
# FSx for Lustre ë§ˆìš´íŠ¸ í™•ì¸
./scripts/check-fsx.sh

# Slurm ì—°ê²° í™•ì¸
./scripts/check-munged.sh

# Pyxis/Enroot ì„¤ì¹˜ í™•ì¸ (ì»¨í…Œì´ë„ˆ ì‚¬ìš© ì‹œ)
./scripts/check-pyxis-enroot.sh
```

### 3. í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

```bash
# Pyxis ë° Enroot ì„¤ì¹˜ (ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„)
sudo ./scripts/install-pyxis-enroot.sh

# NCCL ë° EFA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
./scripts/install-nccl-efa.sh
```

## ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬

### FSDP (Fully Sharded Data Parallel)

PyTorch ë„¤ì´í‹°ë¸Œ ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¡œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- PyTorch í‘œì¤€ APIì™€ ì›í™œí•œ í†µí•©
- ìœ ì—°í•œ ìƒ¤ë”© ì „ëµ (FULL_SHARD, SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD)
- Activation checkpointing ë° CPU offloading
- HuggingFace ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±

**ì‹œì‘í•˜ê¸°:**
```bash
cd fsdp

# ë‹¨ì¼ GPU í…ŒìŠ¤íŠ¸
sbatch train-fsdp-singlegpu.sbatch

# ë©€í‹°ë…¸ë“œ í•™ìŠµ
sbatch train-fsdp.sbatch
```

**ìƒì„¸ ê°€ì´ë“œ:** [fsdp/README.md](fsdp/README.md)

### FSDP2 (Fully Sharded Data Parallel v2)

PyTorch 2.5+ì—ì„œ ë„ì…ëœ ì°¨ì„¸ëŒ€ FSDPë¡œ, í–¥ìƒëœ ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ê°œì„ ëœ í†µì‹  ì˜¤ë²„í—¤ë“œ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ë” ë‚˜ì€ ì»´íŒŒì¼ëŸ¬ ìµœì í™” ì§€ì›
- í–¥ìƒëœ ì²´í¬í¬ì¸íŠ¸ ë° ì¬ì‹œì‘ ê¸°ëŠ¥
- Float8 ì–‘ìí™” ì§€ì›

**ì‹œì‘í•˜ê¸°:**
```bash
cd fsdp2

# ë‹¨ì¼ ë…¸ë“œ í•™ìŠµ
./train-fsdp2-singlenode.sh

# ë©€í‹°ë…¸ë“œ í•™ìŠµ
sbatch train-fsdp2.sbatch
```

**ìƒì„¸ ê°€ì´ë“œ:** [fsdp2/README.md](fsdp2/README.md)

### DeepSpeed

Microsoftì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ZeRO (Zero Redundancy Optimizer) ë‹¨ê³„ë³„ ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ attention êµ¬í˜„
- CPU/NVMe offloading ì§€ì›
- ìë™ í˜¼í•© ì •ë°€ë„ ë° gradient clipping

**ì‹œì‘í•˜ê¸°:**
```bash
cd deepspeed

# Qwen 3 0.6B ë‹¨ì¼ ë…¸ë“œ í•™ìŠµ
./train-qwen3-0-6b-singlenode.sh

# Qwen 3 0.6B ë©€í‹°ë…¸ë“œ í•™ìŠµ
sbatch train-qwen3-0-6b.sbatch
```

**ìƒì„¸ ê°€ì´ë“œ:** [deepspeed/README.md](deepspeed/README.md)

### Megatron-LM

NVIDIAì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- Tensor Parallel, Pipeline Parallel, Data Parallel
- Sequence Parallel ë° Group-Query Attention
- ìµœì í™”ëœ Transformer êµ¬í˜„
- GPT, LLaMA ëª¨ë¸ ì§€ì›

**ì‹œì‘í•˜ê¸°:**
```bash
cd megatron

# ë°ì´í„° ì „ì²˜ë¦¬
sbatch 1.data-preprocessing.sbatch

# ë¶„ì‚° í•™ìŠµ
sbatch 2.distributed-training.sbatch
```

**ìƒì„¸ ê°€ì´ë“œ:** [megatron/megatron-lm-slurm-guide-ko.md](megatron/megatron-lm-slurm-guide-ko.md)

### TorchTitan

Meta(PyTorch íŒ€)ì—ì„œ ê°œë°œí•œ ìµœì‹  ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ í”Œë«í¼ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- FSDP2 (Fully Sharded Data Parallel v2)
- Tensor/Pipeline/Context Parallel
- Float8 ì–‘ìí™” ë° torch.compile í†µí•©
- Zero-bubble Pipeline Parallel
- TensorBoard ë° Weights & Biases í†µí•©

**ì‹œì‘í•˜ê¸°:**
```bash
cd torchtitan

# ë©€í‹°ë…¸ë“œ í•™ìŠµ
sbatch torchtitan-multinode.sbatch

# ì»¤ìŠ¤í…€ ì„¤ì • íŒŒì¼ ì‚¬ìš©
CONFIG_FILE="./custom_config.toml" sbatch torchtitan-multinode.sbatch
```

**ìƒì„¸ ê°€ì´ë“œ:** [torchtitan/torchtitan-guide-ko.md](torchtitan/torchtitan-guide-ko.md)

## ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸

### í´ëŸ¬ìŠ¤í„° ì—°ê²°

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… | ì‚¬ìš©ë²• |
|---------|------|-------|
| `hyperpod-connect.sh` | SSM ê¸°ë°˜ HyperPod ì—°ê²° | `./scripts/hyperpod-connect.sh [--cluster-name NAME]` |
| `hyperpod-ssh.sh` | SSH ê¸°ë°˜ HyperPod ì—°ê²° | `./scripts/hyperpod-ssh.sh --cluster-name NAME` |

### ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… | ì‚¬ìš©ë²• |
|---------|------|-------|
| `check-fsx.sh` | FSx for Lustre ë§ˆìš´íŠ¸ í™•ì¸ | `./scripts/check-fsx.sh` |
| `check-munged.sh` | Slurm ì—°ê²° í™•ì¸ | `./scripts/check-munged.sh` |
| `check-pyxis-enroot.sh` | Pyxis/Enroot ì„¤ì¹˜ í™•ì¸ | `./scripts/check-pyxis-enroot.sh` |

### ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… | ì‚¬ìš©ë²• |
|---------|------|-------|
| `install-pyxis-enroot.sh` | Pyxis/Enroot ì„¤ì¹˜ | `sudo ./scripts/install-pyxis-enroot.sh` |
| `install-nccl-efa.sh` | NCCL ë° EFA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ | `./scripts/install-nccl-efa.sh` |

### ê¸°íƒ€ ë„êµ¬

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… | ì‚¬ìš©ë²• |
|---------|------|-------|
| `fix-cuda-version.sh` | CUDA ë²„ì „ í™•ì¸ ë° ìˆ˜ì • | `./scripts/fix-cuda-version.sh` |
| `generate-nccl-test.sh` | NCCL í…ŒìŠ¤íŠ¸ ìƒì„± | `./scripts/generate-nccl-test.sh` |

## NCCL í…ŒìŠ¤íŠ¸

í´ëŸ¬ìŠ¤í„°ì˜ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:

```bash
# NCCL í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
./scripts/generate-nccl-test.sh

# ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
sbatch nccl-test.sbatch
```

## í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

ëŒ€ë¶€ë¶„ì˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ëŠ” AWS EFA ë° NCCL ìµœì í™”ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤:

```bash
# EFA ì„¤ì •
export FI_PROVIDER=efa
export FI_EFA_USE_HUGE_PAGE=0
export FI_EFA_SET_CUDA_SYNC_MEMOPS=0

# NCCL ì„¤ì •
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=^docker,lo,veth,eth

# CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬
export LD_PRELOAD=/usr/local/cuda-12.8/lib/libnccl.so

# HuggingFace íƒ€ì„ì•„ì›ƒ
export HF_HUB_ETAG_TIMEOUT=60
```

## HyperPod ìë™ ì¬ì‹œì‘

ëª¨ë“  í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ëŠ” HyperPodì˜ ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ë…¸ë“œ ì¥ì•  ì‹œ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.

```bash
# ìë™ ì¬ì‹œì‘ í™œì„±í™” (ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨ë¨)
if [ -d "/opt/sagemaker_cluster" ]; then
    AUTO_RESUME="--auto-resume=1"
fi
```

## ì¼ë°˜ì ì¸ Slurm ëª…ë ¹ì–´

```bash
# ì‘ì—… ì œì¶œ
sbatch script.sbatch

# ì‘ì—… ìƒíƒœ í™•ì¸
squeue
squeue -u $USER

# ì‘ì—… ì·¨ì†Œ
scancel <JOB_ID>

# ë…¸ë“œ ì •ë³´
sinfo
sinfo -N -l

# ì‘ì—… ìƒì„¸ ì •ë³´
scontrol show job <JOB_ID>

# íŒŒí‹°ì…˜ ì •ë³´
scontrol show partition
```

## ë¬¸ì œ í•´ê²°

### ì—°ê²° ë¬¸ì œ

```bash
# SSM ì„¸ì…˜ì´ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°
aws ssm describe-instance-information

# SSH ì—°ê²°ì´ ì•ˆ ë˜ëŠ” ê²½ìš°
# ë³´ì•ˆ ê·¸ë£¹ì—ì„œ SSH í¬íŠ¸(22) í—ˆìš© í™•ì¸
```

### Slurm ë¬¸ì œ

```bash
# Slurm ë°ëª¬ ìƒíƒœ í™•ì¸
sudo systemctl status slurmd

# Slurm ì»¨íŠ¸ë¡¤ëŸ¬ í™•ì¸
sudo systemctl status slurmctld

# ë¡œê·¸ í™•ì¸
sudo journalctl -u slurmd -f
```

### ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

```bash
# EFA ë“œë¼ì´ë²„ í™•ì¸
fi_info -p efa

# ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸
ifconfig

# NCCL í…ŒìŠ¤íŠ¸
./scripts/generate-nccl-test.sh
sbatch nccl-test.sbatch
```

### GPU ë¬¸ì œ

```bash
# GPU ìƒíƒœ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# CUDA ë²„ì „ ìˆ˜ì • (í•„ìš”ì‹œ)
./scripts/fix-cuda-version.sh
```

## ëª¨ë²” ì‚¬ë¡€

1. **ê³µìœ  íŒŒì¼ì‹œìŠ¤í…œ ì‚¬ìš©**: ëª¨ë“  ë…¸ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ FSx for Lustre ì‚¬ìš©
2. **ì²´í¬í¬ì¸íŠ¸ ì €ì¥**: ì •ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ì—¬ ì¥ì•  ë³µêµ¬ ì‹œê°„ ìµœì†Œí™”
3. **ë¡œê·¸ ê´€ë¦¬**: ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³  ì ì ˆí•œ ê¶Œí•œ ì„¤ì •
4. **í™˜ê²½ ê²€ì¦**: í•™ìŠµ ì‹œì‘ ì „ í™˜ê²½ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
5. **ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§**: `squeue`, `nvidia-smi`, `htop` ë“±ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
6. **ë°°ì¹˜ í¬ê¸° ìµœì í™”**: GPU ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ë„ë¡ ë°°ì¹˜ í¬ê¸° ì¡°ì •
7. **ìë™ ì¬ì‹œì‘ í™œìš©**: HyperPodì˜ ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ

## ì„±ëŠ¥ ìµœì í™” íŒ

### ë„¤íŠ¸ì›Œí¬ ìµœì í™”

- EFA ì‚¬ìš© (Enhanced Networking)
- NCCL ë²„í¼ í¬ê¸° ì¡°ì •: `export NCCL_BUFFSIZE=2097152`
- ë¹„ë™ê¸° ì—ëŸ¬ ì²˜ë¦¬: `export NCCL_ASYNC_ERROR_HANDLING=1`

### ë©”ëª¨ë¦¬ ìµœì í™”

- Activation checkpointing í™œì„±í™”
- í˜¼í•© ì •ë°€ë„ í•™ìŠµ (BF16/FP16)
- Gradient accumulationìœ¼ë¡œ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€
- í•„ìš”ì‹œ CPU offloading (ì„±ëŠ¥ ì €í•˜ ì£¼ì˜)

### ì—°ì‚° ìµœì í™”

- torch.compile ì‚¬ìš© (PyTorch 2.0+)
- Flash Attention í™œìš©
- ìµœì ì˜ ë³‘ë ¬í™” ì „ëµ ì„ íƒ (TP, PP, DP ì¡°í•©)

## ê¸°ì—¬í•˜ê¸°

ì´ìŠˆ ë° í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ì£¼ì˜ì‚¬í•­

- `src/legacy` ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì€ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”
- `main` ë¸Œëœì¹˜ì— ì§ì ‘ ì»¤ë°‹í•˜ì§€ ë§ˆì„¸ìš”
- ìƒˆë¡œìš´ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€ ì‹œ ì‹¤í–‰ ê¶Œí•œ ì„¤ì •: `chmod +x script.sh`

## ë¼ì´ì„¼ìŠ¤

MIT-0 License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### AWS ë¬¸ì„œ
- [SageMaker HyperPod ë¬¸ì„œ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html)
- [AWS ParallelCluster ê°€ì´ë“œ](https://docs.aws.amazon.com/parallelcluster/)
- [FSx for Lustre ë¬¸ì„œ](https://docs.aws.amazon.com/fsx/latest/LustreGuide/)
- [EFA ì‚¬ìš©ì ê°€ì´ë“œ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html)

### ë¶„ì‚° í•™ìŠµ ë¦¬ì†ŒìŠ¤
- [PyTorch ë¶„ì‚° í•™ìŠµ ë¬¸ì„œ](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [PyTorch FSDP ë¬¸ì„œ](https://pytorch.org/docs/stable/fsdp.html)
- [Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM)
- [TorchTitan GitHub](https://github.com/pytorch/torchtitan)

### Slurm ë¦¬ì†ŒìŠ¤
- [Slurm ê³µì‹ ë¬¸ì„œ](https://slurm.schedmd.com/)
- [Pyxis GitHub](https://github.com/NVIDIA/pyxis)
- [Enroot GitHub](https://github.com/NVIDIA/enroot)

